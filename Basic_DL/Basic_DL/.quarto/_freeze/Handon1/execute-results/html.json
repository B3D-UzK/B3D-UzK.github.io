{
  "hash": "6215836ed03a725183615ef6327be676",
  "result": {
    "markdown": "The process of creating a neural network using PyTorch with three hidden layers to fit a complex function.\nWe'll use a synthetic dataset for demonstration purposes.\nHere's a step-by-step approach:\n\n1. **Import Libraries:**\n   First, we need to import the necessary libraries.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n### Step 1: Import Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n2. **Create Synthetic Data:**\n   Generate some synthetic data that represents a complex function. For this example, we'll use a sine wave with added noise.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n### Step 2: Create Synthetic Data\n\n# Generate synthetic data\nnp.random.seed(42)\nx = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\ny = np.sin(x) + 0.1 * np.random.normal(size=x.shape)\n\n# Convert to PyTorch tensors\nx_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nplt.plot(x,y)\n```\n\n::: {.cell-output .cell-output-display}\n![](Handon1_files/figure-html/cell-4-output-1.png){width=582 height=411}\n:::\n:::\n\n\n3. **Define the Neural Network:**\n   Create a neural network with three hidden layers.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n### Step 3: Define the Neural Network\nrelu=True #none-linear activation function\n#relu=False #none-linear activation function\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(1, 64)\n        self.layer2 = nn.Linear(64, 128)\n        self.layer3 = nn.Linear(128, 64)\n        self.layer4 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n      if relu:\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.relu(self.layer3(x))\n        x = self.layer4(x)\n        return x\n      else:\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\nmodel = NeuralNetwork()\n```\n:::\n\n\n4. **Define Loss and Optimizer:**\n   Choose a loss function and an optimizer.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n### Step 4: Define Loss and Optimizer\n\ncriterion = nn.MSELoss() #Mean Square Error(MSE) or L2 loss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n#Adam (Adaptive Moment Estimation) optimizer computes adaptive learning rates for each parameter.\n#lr=0.001: This sets the learning rate for the optimizer\n```\n:::\n\n\n5. **Train the Model:**\n   Train the neural network on the synthetic data.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n### Step 5: Train the Model\n\nnum_epochs = 1000\nlosses = []\n\nfor epoch in range(num_epochs):\n    model.train() #Sets the model to training mode\n    optimizer.zero_grad() #Clears old gradients to prevent accumulation from previous iterations.\n    outputs = model(x_tensor) #Performs a forward pass, generating predictions from the input data.\n    loss = criterion(outputs, y_tensor) # Calculates the loss (error) between the predictions and true targets.\n    loss.backward()  #Computes gradients of the loss with respect to the model parameters (backpropagation).\n    optimizer.step() #Updates the model parameters using the computed gradients to minimize the loss.\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [100/1000], Loss: 0.0591\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [200/1000], Loss: 0.0129\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [300/1000], Loss: 0.0099\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [400/1000], Loss: 0.0098\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [500/1000], Loss: 0.0094\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [600/1000], Loss: 0.0107\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [700/1000], Loss: 0.0092\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [800/1000], Loss: 0.0091\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [900/1000], Loss: 0.0091\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1000/1000], Loss: 0.0097\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Handon1_files/figure-html/cell-8-output-1.png){width=589 height=449}\n:::\n:::\n\n\n6. **Evaluate the Model:**\n   Evaluate the model's performance.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n### Step 6: Evaluate the Model\n\nmodel.eval()\nwith torch.no_grad():\n    predicted = model(x_tensor).numpy()\n\n# Plot the results\nplt.plot(x, y, label='Original Data')\nplt.plot(x, predicted, label='Fitted Data')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Handon1_files/figure-html/cell-9-output-1.png){width=582 height=411}\n:::\n:::\n\n\n",
    "supporting": [
      "Handon1_files"
    ],
    "filters": [],
    "includes": {}
  }
}