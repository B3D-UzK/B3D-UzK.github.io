{
  "hash": "6d61c151464a03c03ac98e42a7386a77",
  "result": {
    "markdown": "::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Define a transform to normalize the data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Load the training and test datasets\ntrainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\ntestset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef imshow(img):\n    img = img / 2 + 0.5  # Unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n# Get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Plot 20 images in a 4x5 grid\nfig, axes = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    if idx < len(images):\n        ax.imshow(images[idx].numpy().squeeze(), cmap='gray')\n        ax.set_title(f'Label: {labels[idx].item()}')\n        ax.axis('off')\n    else:\n        ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Handon4_files/figure-html/cell-4-output-1.png){width=950 height=911}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Define the Neural Network\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(28*28, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28*28)  # Flatten the image\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\nnet = Net()\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Define the Loss Function and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Lists to keep track of losses\ntrain_losses = []\ntest_losses = []\n\nfor epoch in range(10):  # Loop over the dataset multiple times\n    running_loss = 0.0\n    net.train()  # Set the network to training mode\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:    # Print every 100 mini-batches\n            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n            running_loss = 0.0\n\n    # Track training loss\n    train_losses.append(loss.item())\n\n    # Track test loss\n    net.eval()  # Set the network to evaluation mode\n    test_loss = 0.0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            outputs = net(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n    test_losses.append(test_loss / len(testloader))\n\nprint('Finished Training')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 1, Batch 100] loss: 0.808\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 1, Batch 200] loss: 0.413\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 1, Batch 300] loss: 0.337\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 1, Batch 400] loss: 0.277\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 1, Batch 500] loss: 0.244\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 1, Batch 600] loss: 0.199\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 1, Batch 700] loss: 0.198\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 1, Batch 800] loss: 0.188\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 1, Batch 900] loss: 0.190\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 2, Batch 100] loss: 0.155\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 2, Batch 200] loss: 0.150\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 2, Batch 300] loss: 0.140\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 2, Batch 400] loss: 0.151\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 2, Batch 500] loss: 0.137\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 2, Batch 600] loss: 0.131\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 2, Batch 700] loss: 0.133\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 2, Batch 800] loss: 0.149\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 2, Batch 900] loss: 0.137\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 3, Batch 100] loss: 0.112\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 3, Batch 200] loss: 0.107\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 3, Batch 300] loss: 0.107\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 3, Batch 400] loss: 0.105\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 3, Batch 500] loss: 0.115\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 3, Batch 600] loss: 0.117\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 3, Batch 700] loss: 0.106\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 3, Batch 800] loss: 0.127\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 3, Batch 900] loss: 0.106\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 4, Batch 100] loss: 0.075\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 4, Batch 200] loss: 0.092\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 4, Batch 300] loss: 0.078\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 4, Batch 400] loss: 0.095\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 4, Batch 500] loss: 0.094\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 4, Batch 600] loss: 0.091\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 4, Batch 700] loss: 0.096\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 4, Batch 800] loss: 0.091\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 4, Batch 900] loss: 0.077\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 5, Batch 100] loss: 0.070\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 5, Batch 200] loss: 0.084\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 5, Batch 300] loss: 0.070\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 5, Batch 400] loss: 0.081\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 5, Batch 500] loss: 0.077\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 5, Batch 600] loss: 0.062\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 5, Batch 700] loss: 0.076\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 5, Batch 800] loss: 0.076\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 5, Batch 900] loss: 0.065\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 6, Batch 100] loss: 0.067\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 6, Batch 200] loss: 0.067\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 6, Batch 300] loss: 0.055\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 6, Batch 400] loss: 0.076\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 6, Batch 500] loss: 0.080\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 6, Batch 600] loss: 0.061\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 6, Batch 700] loss: 0.066\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 6, Batch 800] loss: 0.063\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 6, Batch 900] loss: 0.076\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 7, Batch 100] loss: 0.043\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 7, Batch 200] loss: 0.068\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 7, Batch 300] loss: 0.058\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 7, Batch 400] loss: 0.060\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 7, Batch 500] loss: 0.073\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 7, Batch 600] loss: 0.053\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 7, Batch 700] loss: 0.060\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 7, Batch 800] loss: 0.067\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 7, Batch 900] loss: 0.063\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 8, Batch 100] loss: 0.047\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 8, Batch 200] loss: 0.052\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 8, Batch 300] loss: 0.052\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 8, Batch 400] loss: 0.057\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 8, Batch 500] loss: 0.060\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 8, Batch 600] loss: 0.053\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 8, Batch 700] loss: 0.056\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 8, Batch 800] loss: 0.053\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 8, Batch 900] loss: 0.057\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 9, Batch 100] loss: 0.038\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 9, Batch 200] loss: 0.037\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 9, Batch 300] loss: 0.055\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 9, Batch 400] loss: 0.046\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 9, Batch 500] loss: 0.065\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 9, Batch 600] loss: 0.047\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 9, Batch 700] loss: 0.049\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 9, Batch 800] loss: 0.042\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 9, Batch 900] loss: 0.052\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 10, Batch 100] loss: 0.038\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 10, Batch 200] loss: 0.044\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 10, Batch 300] loss: 0.038\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 10, Batch 400] loss: 0.036\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 10, Batch 500] loss: 0.053\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 10, Batch 600] loss: 0.049\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 10, Batch 700] loss: 0.044\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 10, Batch 800] loss: 0.050\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch 10, Batch 900] loss: 0.045\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFinished Training\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Plot the training and test losses\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Test Loss')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Handon4_files/figure-html/cell-8-output-1.png){width=829 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Test the Network\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of the network on the 10000 test images: 97.69%\n```\n:::\n:::\n\n\n",
    "supporting": [
      "Handon4_files"
    ],
    "filters": [],
    "includes": {}
  }
}