{"title":"Step 1: Import Libraries","markdown":{"headingText":"Step 1: Import Libraries","containsRefs":false,"markdown":"\nThe process of creating a neural network using PyTorch with three hidden layers to fit a complex function.\nWe'll use a synthetic dataset for demonstration purposes.\nHere's a step-by-step approach:\n\n1. **Import Libraries:**\n   First, we need to import the necessary libraries.\n   \n```{python}\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n\n2. **Create Synthetic Data:**\n   Generate some synthetic data that represents a complex function. For this example, we'll use a sine wave with added noise.\n\n\n```{python}\n### Step 2: Create Synthetic Data\n\n# Generate synthetic data\nnp.random.seed(42)\nx = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\ny = np.sin(x) + 0.1 * np.random.normal(size=x.shape)\n\n# Convert to PyTorch tensors\nx_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n```\n\n\n\n```{python}\nplt.plot(x,y)\n```\n\n3. **Define the Neural Network:**\n   Create a neural network with three hidden layers.\n\n\n\n```{python}\n### Step 3: Define the Neural Network\nrelu=True #none-linear activation function\n#relu=False #none-linear activation function\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(1, 64)\n        self.layer2 = nn.Linear(64, 128)\n        self.layer3 = nn.Linear(128, 64)\n        self.layer4 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n      if relu:\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.relu(self.layer3(x))\n        x = self.layer4(x)\n        return x\n      else:\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\nmodel = NeuralNetwork()\n\n```\n\n4. **Define Loss and Optimizer:**\n   Choose a loss function and an optimizer.\n\n\n```{python}\n### Step 4: Define Loss and Optimizer\n\ncriterion = nn.MSELoss() #Mean Square Error(MSE) or L2 loss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n#Adam (Adaptive Moment Estimation) optimizer computes adaptive learning rates for each parameter.\n#lr=0.001: This sets the learning rate for the optimizer\n```\n\n5. **Train the Model:**\n   Train the neural network on the synthetic data.\n\n\n\n\n```{python}\n### Step 5: Train the Model\n\nnum_epochs = 1000\nlosses = []\n\nfor epoch in range(num_epochs):\n    model.train() #Sets the model to training mode\n    optimizer.zero_grad() #Clears old gradients to prevent accumulation from previous iterations.\n    outputs = model(x_tensor) #Performs a forward pass, generating predictions from the input data.\n    loss = criterion(outputs, y_tensor) # Calculates the loss (error) between the predictions and true targets.\n    loss.backward()  #Computes gradients of the loss with respect to the model parameters (backpropagation).\n    optimizer.step() #Updates the model parameters using the computed gradients to minimize the loss.\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n```\n\n\n\n```{python}\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n```\n\n\n6. **Evaluate the Model:**\n   Evaluate the model's performance.\n\n```{python}\n### Step 6: Evaluate the Model\n\nmodel.eval()\nwith torch.no_grad():\n    predicted = model(x_tensor).numpy()\n\n# Plot the results\nplt.plot(x, y, label='Original Data')\nplt.plot(x, predicted, label='Fitted Data')\nplt.legend()\nplt.show()\n```\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"Handon1.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","editor":"visual","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}}}