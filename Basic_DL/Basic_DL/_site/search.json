[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basic_DL",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Handon1.html",
    "href": "Handon1.html",
    "title": "Basic_DL",
    "section": "",
    "text": "The process of creating a neural network using PyTorch with three hidden layers to fit a complex function. We’ll use a synthetic dataset for demonstration purposes. Here’s a step-by-step approach:\n\nImport Libraries: First, we need to import the necessary libraries.\n\n\n### Step 1: Import Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nCreate Synthetic Data: Generate some synthetic data that represents a complex function. For this example, we’ll use a sine wave with added noise.\n\n\n### Step 2: Create Synthetic Data\n\n# Generate synthetic data\nnp.random.seed(42)\nx = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\ny = np.sin(x) + 0.1 * np.random.normal(size=x.shape)\n\n# Convert to PyTorch tensors\nx_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n\nplt.plot(x,y)\n\n\n\n\n\nDefine the Neural Network: Create a neural network with three hidden layers.\n\n\n### Step 3: Define the Neural Network\nrelu=True #none-linear activation function\n#relu=False #none-linear activation function\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(1, 64)\n        self.layer2 = nn.Linear(64, 128)\n        self.layer3 = nn.Linear(128, 64)\n        self.layer4 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n      if relu:\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.relu(self.layer3(x))\n        x = self.layer4(x)\n        return x\n      else:\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\nmodel = NeuralNetwork()\n\n\nDefine Loss and Optimizer: Choose a loss function and an optimizer.\n\n\n### Step 4: Define Loss and Optimizer\n\ncriterion = nn.MSELoss() #Mean Square Error(MSE) or L2 loss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n#Adam (Adaptive Moment Estimation) optimizer computes adaptive learning rates for each parameter.\n#lr=0.001: This sets the learning rate for the optimizer\n\n\nTrain the Model: Train the neural network on the synthetic data.\n\n\n### Step 5: Train the Model\n\nnum_epochs = 1000\nlosses = []\n\nfor epoch in range(num_epochs):\n    model.train() #Sets the model to training mode\n    optimizer.zero_grad() #Clears old gradients to prevent accumulation from previous iterations.\n    outputs = model(x_tensor) #Performs a forward pass, generating predictions from the input data.\n    loss = criterion(outputs, y_tensor) # Calculates the loss (error) between the predictions and true targets.\n    loss.backward()  #Computes gradients of the loss with respect to the model parameters (backpropagation).\n    optimizer.step() #Updates the model parameters using the computed gradients to minimize the loss.\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n\nEpoch [100/1000], Loss: 0.0591\n\n\nEpoch [200/1000], Loss: 0.0129\n\n\nEpoch [300/1000], Loss: 0.0099\n\n\nEpoch [400/1000], Loss: 0.0098\n\n\nEpoch [500/1000], Loss: 0.0094\n\n\nEpoch [600/1000], Loss: 0.0107\n\n\nEpoch [700/1000], Loss: 0.0092\n\n\nEpoch [800/1000], Loss: 0.0091\n\n\nEpoch [900/1000], Loss: 0.0091\n\n\nEpoch [1000/1000], Loss: 0.0097\n\n\n\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n\n\n\n\n\nEvaluate the Model: Evaluate the model’s performance.\n\n\n### Step 6: Evaluate the Model\n\nmodel.eval()\nwith torch.no_grad():\n    predicted = model(x_tensor).numpy()\n\n# Plot the results\nplt.plot(x, y, label='Original Data')\nplt.plot(x, predicted, label='Fitted Data')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Handon2.html",
    "href": "Handon2.html",
    "title": "Basic_DL",
    "section": "",
    "text": "We’ll create a neural network with three hidden layers to fit the data. Here’s the step-by-step approach:"
  },
  {
    "objectID": "Handon2.html#the-architecture-of-the-neural-network",
    "href": "Handon2.html#the-architecture-of-the-neural-network",
    "title": "Basic_DL",
    "section": "The architecture of the neural network",
    "text": "The architecture of the neural network\n\nInput Size = 2\nThe input size of 2 is determined by the nature of the dataset we’re working with. The make_circles function from sklearn.datasets generates a 2-dimensional dataset where each data point has two features (coordinates in a 2D space). Therefore, the input to the neural network has to accommodate these two features, which is why the first layer (nn.Linear(2, 64)) expects an input size of 2.\n\n\nOutput Size = 1\nThe output size of 1 is determined by the task we’re trying to solve, which is a binary classification problem. The make_circles dataset labels each data point as either 0 or 1. Thus, the neural network needs to output a single value for each data point that can be interpreted as the probability of the data point belonging to class 1. The final layer (nn.Linear(64, 1)) reduces the output to a single value for each input data point.\n\n\nNetwork Architecture\nHere’s a brief breakdown of the network layers:\n\nInput Layer: self.layer1 = nn.Linear(2, 64)\n\nTakes 2 input features (the 2D coordinates).\nOutputs 64 features.\nActivation function: ReLU (self.relu).\n\nFirst Hidden Layer: self.layer2 = nn.Linear(64, 128)\n\nTakes the 64 features from the previous layer.\nOutputs 128 features.\nActivation function: ReLU.\n\nSecond Hidden Layer: self.layer3 = nn.Linear(128, 64)\n\nTakes the 128 features from the previous layer.\nOutputs 64 features.\nActivation function: ReLU.\n\nOutput Layer: self.layer4 = nn.Linear(64, 1)\n\nTakes the 64 features from the previous layer.\nOutputs a single feature, representing the predicted probability of the input belonging to class 1.\n\n\n\n\nActivation Function: ReLU\nThe ReLU (Rectified Linear Unit) activation function is applied after each hidden layer to introduce non-linearity into the model, allowing it to learn more complex patterns.\n\n# Step 3: Define the Neural Network\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(2, 64)\n        self.layer2 = nn.Linear(64, 128)\n        self.layer3 = nn.Linear(128, 64)\n        self.layer4 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.relu(self.layer3(x))\n        x = self.layer4(x)\n        return x\n\nmodel = NeuralNetwork()\n\n\n# Step 4: Define Loss and Optimizer\n\ncriterion = nn.BCEWithLogitsLoss()\n#BCEWithLogitsLoss: Binary Cross Entropy with Logits Loss which is suitable for binary classification tasks\n    #•  Binary Cross Entropy (BCE) Loss: Measures the difference between true labels and predicted probabilities.\n    #•  With Logits: Expects raw output (logits) from the model, not probabilities. Internally applies the sigmoid function to logits before computing the loss.\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n#Adam (Adaptive Moment Estimation) optimizer computes adaptive learning rates for each parameter.\n#lr=0.001: This sets the learning rate for the optimizer\n\n\n# Step 5: Train the Model\n\nnum_epochs = 1000\nlosses = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n\nEpoch [100/1000], Loss: 0.0096\n\n\nEpoch [200/1000], Loss: 0.0015\n\n\nEpoch [300/1000], Loss: 0.0005\n\n\nEpoch [400/1000], Loss: 0.0003\n\n\nEpoch [500/1000], Loss: 0.0002\n\n\nEpoch [600/1000], Loss: 0.0001\n\n\nEpoch [700/1000], Loss: 0.0001\n\n\nEpoch [800/1000], Loss: 0.0001\n\n\nEpoch [900/1000], Loss: 0.0001\n\n\nEpoch [1000/1000], Loss: 0.0000\n\n\n\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n\n\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    train_outputs = model(X_train)\n    test_outputs = model(X_test)\n    train_pred = torch.round(torch.sigmoid(train_outputs))\n    test_pred = torch.round(torch.sigmoid(test_outputs))\n\n    train_accuracy = (train_pred.eq(y_train).sum() / float(y_train.shape[0])).item()\n    test_accuracy = (test_pred.eq(y_test).sum() / float(y_test.shape[0])).item()\n\nprint(f'Train Accuracy: {train_accuracy*100:.2f}%')\nprint(f'Test Accuracy: {test_accuracy*100:.2f}%')\n\n# Plot the results\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.title('Train Data')\nplt.scatter(X_train[:, 0], X_train[:, 1], c=train_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.subplot(1, 2, 2)\nplt.title('Test Data')\nplt.scatter(X_test[:, 0], X_test[:, 1], c=test_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.show()\n\nTrain Accuracy: 100.00%\nTest Accuracy: 100.00%\n\n\n\n\n\n\n# Function 1: plot decision boundary\n\ndef plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n    \"\"\"\n    Plots decision boundaries of model predictions on X in comparison to y.\n    \"\"\"\n    # Move model and data to CPU\n    model.to(\"cpu\")\n    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n\n    # Setup grid for plotting decision boundaries\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n\n    # Prepare data for prediction\n    X_to_pred_on = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n\n    # Make predictions\n    model.eval()\n    with torch.no_grad():\n        y_logits = model(X_to_pred_on)\n\n    # Adjust logits to prediction labels\n    if len(torch.unique(y)) > 2:\n        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # multi-class\n    else:\n        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n\n    # Reshape predictions and plot\n    y_pred = y_pred.reshape(xx.shape).numpy()\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.show()\n\n# Function 2: plot predictions\ndef plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n    \"\"\"\n    Plots training and test data and compares predictions if provided.\n    \"\"\"\n    plt.figure(figsize=(10, 7))\n\n    # Plot training data\n    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n    # Plot test data\n    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n    if predictions is not None:\n        # Plot predictions on test data\n        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n    # Display legend\n    plt.legend(prop={\"size\": 14})\n    plt.show()\n\n\n# Plot decision boundaries for training and test sets\nplt.figure(figsize=(6, 6))\nplt.title(\"Train\")\nplot_decision_boundary(model, X_train, y_train)\n\nplt.figure(figsize=(6, 6))\nplt.title(\"Test\")\nplot_decision_boundary(model, X_test, y_test)"
  },
  {
    "objectID": "Handon3.html",
    "href": "Handon3.html",
    "title": "Basic_DL",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Suppress all RuntimeWarnings\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\n# Generate the dataset\nX, y = make_moons(n_samples=1000, noise=0.03, random_state=42)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert the data to PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Change to (N, 1)\ny_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)  # Change to (N, 1)\n\n\n# Define the neural network model\nclass MoonModel(nn.Module):\n    def __init__(self):\n        super(MoonModel, self).__init__()\n        self.layer_1 = nn.Linear(2, 10)\n        self.layer_2 = nn.Linear(10, 10)\n        self.layer_3 = nn.Linear(10, 10)\n        self.output = nn.Linear(10, 1)  # Single output neuron\n\n    def forward(self, x):\n        x = torch.relu(self.layer_1(x))\n        x = torch.relu(self.layer_2(x))\n        x = torch.relu(self.layer_3(x))\n        x = self.output(x)\n        return x\n\nmodel = MoonModel()\n\n\n# Define the loss function and the optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n# Train the model\nepochs = 1000\nlosses = []\n\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n\nEpoch [100/1000], Loss: 0.5655\nEpoch [200/1000], Loss: 0.3441\n\n\nEpoch [300/1000], Loss: 0.1973\nEpoch [400/1000], Loss: 0.1112\n\n\nEpoch [500/1000], Loss: 0.0625\nEpoch [600/1000], Loss: 0.0319\n\n\nEpoch [700/1000], Loss: 0.0166\nEpoch [800/1000], Loss: 0.0096\n\n\nEpoch [900/1000], Loss: 0.0061\n\n\nEpoch [1000/1000], Loss: 0.0041\n\n\n\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n\n\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    train_outputs = model(X_train)\n    test_outputs = model(X_test)\n    train_pred = torch.round(torch.sigmoid(train_outputs))\n    test_pred = torch.round(torch.sigmoid(test_outputs))\n\n    train_accuracy = (train_pred.eq(y_train).sum() / float(y_train.shape[0])).item()\n    test_accuracy = (test_pred.eq(y_test).sum() / float(y_test.shape[0])).item()\n\nprint(f'Train Accuracy: {train_accuracy*100:.2f}%')\nprint(f'Test Accuracy: {test_accuracy*100:.2f}%')\n\n# Plot the results\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.title('Train Data')\nplt.scatter(X_train[:, 0], X_train[:, 1], c=train_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.subplot(1, 2, 2)\nplt.title('Test Data')\nplt.scatter(X_test[:, 0], X_test[:, 1], c=test_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.show()\n\nTrain Accuracy: 100.00%\nTest Accuracy: 100.00%\n\n\n\n\n\n\n# Function 1: plot decision boundary\nimport numpy as np\ndef plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n    \"\"\"\n    Plots decision boundaries of model predictions on X in comparison to y.\n    \"\"\"\n    # Move model and data to CPU\n    model.to(\"cpu\")\n    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n\n    # Setup grid for plotting decision boundaries\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n\n    # Prepare data for prediction\n    X_to_pred_on = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n\n    # Make predictions\n    model.eval()\n    with torch.no_grad():\n        y_logits = model(X_to_pred_on)\n\n    # Adjust logits to prediction labels\n    if len(torch.unique(y)) > 2:\n        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # multi-class\n    else:\n        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n\n    # Reshape predictions and plot\n    y_pred = y_pred.reshape(xx.shape).numpy()\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.show()\n\n# Function 2: plot predictions\ndef plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n    \"\"\"\n    Plots training and test data and compares predictions if provided.\n    \"\"\"\n    #plt.figure(figsize=(10, 7))\n\n    # Plot training data\n    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n    # Plot test data\n    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n    if predictions is not None:\n        # Plot predictions on test data\n        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n    # Display legend\n    plt.legend(prop={\"size\": 14})\n    plt.show()\n\n\n# Plot decision boundaries for training and test sets\nplt.figure(figsize=(6, 6))\nplt.title(\"Train\")\nplot_decision_boundary(model, X_train, y_train)\n\nplt.figure(figsize=(6, 6))\nplt.title(\"Test\")\nplot_decision_boundary(model, X_test, y_test)"
  },
  {
    "objectID": "Handon4.html",
    "href": "Handon4.html",
    "title": "Basic_DL",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Define a transform to normalize the data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Load the training and test datasets\ntrainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\ntestset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\n\ndef imshow(img):\n    img = img / 2 + 0.5  # Unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n# Get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Plot 20 images in a 4x5 grid\nfig, axes = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    if idx < len(images):\n        ax.imshow(images[idx].numpy().squeeze(), cmap='gray')\n        ax.set_title(f'Label: {labels[idx].item()}')\n        ax.axis('off')\n    else:\n        ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# Define the Neural Network\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(28*28, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28*28)  # Flatten the image\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\nnet = Net()\n\n\n# Define the Loss Function and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\n\n# Lists to keep track of losses\ntrain_losses = []\ntest_losses = []\n\nfor epoch in range(10):  # Loop over the dataset multiple times\n    running_loss = 0.0\n    net.train()  # Set the network to training mode\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:    # Print every 100 mini-batches\n            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n            running_loss = 0.0\n\n    # Track training loss\n    train_losses.append(loss.item())\n\n    # Track test loss\n    net.eval()  # Set the network to evaluation mode\n    test_loss = 0.0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            outputs = net(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n    test_losses.append(test_loss / len(testloader))\n\nprint('Finished Training')\n\n[Epoch 1, Batch 100] loss: 0.808\n\n\n[Epoch 1, Batch 200] loss: 0.413\n\n\n[Epoch 1, Batch 300] loss: 0.337\n\n\n[Epoch 1, Batch 400] loss: 0.277\n\n\n[Epoch 1, Batch 500] loss: 0.244\n\n\n[Epoch 1, Batch 600] loss: 0.199\n\n\n[Epoch 1, Batch 700] loss: 0.198\n\n\n[Epoch 1, Batch 800] loss: 0.188\n\n\n[Epoch 1, Batch 900] loss: 0.190\n\n\n[Epoch 2, Batch 100] loss: 0.155\n\n\n[Epoch 2, Batch 200] loss: 0.150\n\n\n[Epoch 2, Batch 300] loss: 0.140\n\n\n[Epoch 2, Batch 400] loss: 0.151\n\n\n[Epoch 2, Batch 500] loss: 0.137\n\n\n[Epoch 2, Batch 600] loss: 0.131\n\n\n[Epoch 2, Batch 700] loss: 0.133\n\n\n[Epoch 2, Batch 800] loss: 0.149\n\n\n[Epoch 2, Batch 900] loss: 0.137\n\n\n[Epoch 3, Batch 100] loss: 0.112\n\n\n[Epoch 3, Batch 200] loss: 0.107\n\n\n[Epoch 3, Batch 300] loss: 0.107\n\n\n[Epoch 3, Batch 400] loss: 0.105\n\n\n[Epoch 3, Batch 500] loss: 0.115\n\n\n[Epoch 3, Batch 600] loss: 0.117\n\n\n[Epoch 3, Batch 700] loss: 0.106\n\n\n[Epoch 3, Batch 800] loss: 0.127\n\n\n[Epoch 3, Batch 900] loss: 0.106\n\n\n[Epoch 4, Batch 100] loss: 0.075\n\n\n[Epoch 4, Batch 200] loss: 0.092\n\n\n[Epoch 4, Batch 300] loss: 0.078\n\n\n[Epoch 4, Batch 400] loss: 0.095\n\n\n[Epoch 4, Batch 500] loss: 0.094\n\n\n[Epoch 4, Batch 600] loss: 0.091\n\n\n[Epoch 4, Batch 700] loss: 0.096\n\n\n[Epoch 4, Batch 800] loss: 0.091\n\n\n[Epoch 4, Batch 900] loss: 0.077\n\n\n[Epoch 5, Batch 100] loss: 0.070\n\n\n[Epoch 5, Batch 200] loss: 0.084\n\n\n[Epoch 5, Batch 300] loss: 0.070\n\n\n[Epoch 5, Batch 400] loss: 0.081\n\n\n[Epoch 5, Batch 500] loss: 0.077\n\n\n[Epoch 5, Batch 600] loss: 0.062\n\n\n[Epoch 5, Batch 700] loss: 0.076\n\n\n[Epoch 5, Batch 800] loss: 0.076\n\n\n[Epoch 5, Batch 900] loss: 0.065\n\n\n[Epoch 6, Batch 100] loss: 0.067\n\n\n[Epoch 6, Batch 200] loss: 0.067\n\n\n[Epoch 6, Batch 300] loss: 0.055\n\n\n[Epoch 6, Batch 400] loss: 0.076\n\n\n[Epoch 6, Batch 500] loss: 0.080\n\n\n[Epoch 6, Batch 600] loss: 0.061\n\n\n[Epoch 6, Batch 700] loss: 0.066\n\n\n[Epoch 6, Batch 800] loss: 0.063\n\n\n[Epoch 6, Batch 900] loss: 0.076\n\n\n[Epoch 7, Batch 100] loss: 0.043\n\n\n[Epoch 7, Batch 200] loss: 0.068\n\n\n[Epoch 7, Batch 300] loss: 0.058\n\n\n[Epoch 7, Batch 400] loss: 0.060\n\n\n[Epoch 7, Batch 500] loss: 0.073\n\n\n[Epoch 7, Batch 600] loss: 0.053\n\n\n[Epoch 7, Batch 700] loss: 0.060\n\n\n[Epoch 7, Batch 800] loss: 0.067\n\n\n[Epoch 7, Batch 900] loss: 0.063\n\n\n[Epoch 8, Batch 100] loss: 0.047\n\n\n[Epoch 8, Batch 200] loss: 0.052\n\n\n[Epoch 8, Batch 300] loss: 0.052\n\n\n[Epoch 8, Batch 400] loss: 0.057\n\n\n[Epoch 8, Batch 500] loss: 0.060\n\n\n[Epoch 8, Batch 600] loss: 0.053\n\n\n[Epoch 8, Batch 700] loss: 0.056\n\n\n[Epoch 8, Batch 800] loss: 0.053\n\n\n[Epoch 8, Batch 900] loss: 0.057\n\n\n[Epoch 9, Batch 100] loss: 0.038\n\n\n[Epoch 9, Batch 200] loss: 0.037\n\n\n[Epoch 9, Batch 300] loss: 0.055\n\n\n[Epoch 9, Batch 400] loss: 0.046\n\n\n[Epoch 9, Batch 500] loss: 0.065\n\n\n[Epoch 9, Batch 600] loss: 0.047\n\n\n[Epoch 9, Batch 700] loss: 0.049\n\n\n[Epoch 9, Batch 800] loss: 0.042\n\n\n[Epoch 9, Batch 900] loss: 0.052\n\n\n[Epoch 10, Batch 100] loss: 0.038\n\n\n[Epoch 10, Batch 200] loss: 0.044\n\n\n[Epoch 10, Batch 300] loss: 0.038\n\n\n[Epoch 10, Batch 400] loss: 0.036\n\n\n[Epoch 10, Batch 500] loss: 0.053\n\n\n[Epoch 10, Batch 600] loss: 0.049\n\n\n[Epoch 10, Batch 700] loss: 0.044\n\n\n[Epoch 10, Batch 800] loss: 0.050\n\n\n[Epoch 10, Batch 900] loss: 0.045\n\n\nFinished Training\n\n\n\n# Plot the training and test losses\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Test Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Test the Network\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n\nAccuracy of the network on the 10000 test images: 97.69%"
  }
]