{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We’ll create a neural network with three hidden layers to fit the data. Here’s the step-by-step approach:\n",
        "\n",
        "\t1.\tImport Libraries\n",
        "\t2.\tCreate the Dataset\n",
        "\t3.\tDefine the Neural Network\n",
        "\t4.\tDefine Loss and Optimizer\n",
        "\t5.\tTrain the Model\n",
        "\t6.\tEvaluate the Model\n"
      ],
      "id": "eb944153"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Import Libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "id": "354fa6b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 2: Create the Dataset\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_circles(n_samples=1000, noise=0.03, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)"
      ],
      "id": "7cdd32da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize with a plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x=X[:, 0],\n",
        "            y=X[:, 1],\n",
        "            c=y,\n",
        "            cmap=plt.cm.RdYlBu);"
      ],
      "id": "b64a0192",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The architecture of the neural network\n",
        "\n",
        "### Input Size = 2\n",
        "\n",
        "The input size of 2 is determined by the nature of the dataset we're working with. The `make_circles` function from `sklearn.datasets` generates a 2-dimensional dataset where each data point has two features (coordinates in a 2D space). Therefore, the input to the neural network has to accommodate these two features, which is why the first layer (`nn.Linear(2, 64)`) expects an input size of 2.\n",
        "\n",
        "### Output Size = 1\n",
        "\n",
        "The output size of 1 is determined by the task we're trying to solve, which is a binary classification problem. The `make_circles` dataset labels each data point as either 0 or 1. Thus, the neural network needs to output a single value for each data point that can be interpreted as the probability of the data point belonging to class 1. The final layer (`nn.Linear(64, 1)`) reduces the output to a single value for each input data point.\n",
        "\n",
        "### Network Architecture\n",
        "\n",
        "Here's a brief breakdown of the network layers:\n",
        "\n",
        "1. **Input Layer**: `self.layer1 = nn.Linear(2, 64)`\n",
        "   - Takes 2 input features (the 2D coordinates).\n",
        "   - Outputs 64 features.\n",
        "   - Activation function: ReLU (`self.relu`).\n",
        "\n",
        "2. **First Hidden Layer**: `self.layer2 = nn.Linear(64, 128)`\n",
        "   - Takes the 64 features from the previous layer.\n",
        "   - Outputs 128 features.\n",
        "   - Activation function: ReLU.\n",
        "\n",
        "3. **Second Hidden Layer**: `self.layer3 = nn.Linear(128, 64)`\n",
        "   - Takes the 128 features from the previous layer.\n",
        "   - Outputs 64 features.\n",
        "   - Activation function: ReLU.\n",
        "\n",
        "4. **Output Layer**: `self.layer4 = nn.Linear(64, 1)`\n",
        "   - Takes the 64 features from the previous layer.\n",
        "   - Outputs a single feature, representing the predicted probability of the input belonging to class 1.\n",
        "\n",
        "### Activation Function: ReLU\n",
        "\n",
        "The ReLU (Rectified Linear Unit) activation function is applied after each hidden layer to introduce non-linearity into the model, allowing it to learn more complex patterns.\n"
      ],
      "id": "50ed5308"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3: Define the Neural Network\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(2, 64)\n",
        "        self.layer2 = nn.Linear(64, 128)\n",
        "        self.layer3 = nn.Linear(128, 64)\n",
        "        self.layer4 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x = self.layer4(x)\n",
        "        return x\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "id": "3990b191",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 4: Define Loss and Optimizer\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "#BCEWithLogitsLoss: Binary Cross Entropy with Logits Loss which is suitable for binary classification tasks\n",
        "\t#•\tBinary Cross Entropy (BCE) Loss: Measures the difference between true labels and predicted probabilities.\n",
        "\t#•\tWith Logits: Expects raw output (logits) from the model, not probabilities. Internally applies the sigmoid function to logits before computing the loss.\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "#Adam (Adaptive Moment Estimation) optimizer computes adaptive learning rates for each parameter.\n",
        "#lr=0.001: This sets the learning rate for the optimizer"
      ],
      "id": "1b0ac7db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 5: Train the Model\n",
        "\n",
        "num_epochs = 1000\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    losses.append(loss.item())"
      ],
      "id": "2fa954ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot the training loss\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ],
      "id": "2ac4ce95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    train_outputs = model(X_train)\n",
        "    test_outputs = model(X_test)\n",
        "    train_pred = torch.round(torch.sigmoid(train_outputs))\n",
        "    test_pred = torch.round(torch.sigmoid(test_outputs))\n",
        "\n",
        "    train_accuracy = (train_pred.eq(y_train).sum() / float(y_train.shape[0])).item()\n",
        "    test_accuracy = (test_pred.eq(y_test).sum() / float(y_test.shape[0])).item()\n",
        "\n",
        "print(f'Train Accuracy: {train_accuracy*100:.2f}%')\n",
        "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Train Data')\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=train_pred[:, 0], cmap='coolwarm', alpha=0.6)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Test Data')\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=test_pred[:, 0], cmap='coolwarm', alpha=0.6)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.show()"
      ],
      "id": "20452a5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function 1: plot decision boundary\n",
        "\n",
        "def plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Plots decision boundaries of model predictions on X in comparison to y.\n",
        "    \"\"\"\n",
        "    # Move model and data to CPU\n",
        "    model.to(\"cpu\")\n",
        "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
        "\n",
        "    # Setup grid for plotting decision boundaries\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
        "\n",
        "    # Prepare data for prediction\n",
        "    X_to_pred_on = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n",
        "\n",
        "    # Make predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_logits = model(X_to_pred_on)\n",
        "\n",
        "    # Adjust logits to prediction labels\n",
        "    if len(torch.unique(y)) > 2:\n",
        "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # multi-class\n",
        "    else:\n",
        "        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n",
        "\n",
        "    # Reshape predictions and plot\n",
        "    y_pred = y_pred.reshape(xx.shape).numpy()\n",
        "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.show()\n",
        "\n",
        "# Function 2: plot predictions\n",
        "def plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n",
        "    \"\"\"\n",
        "    Plots training and test data and compares predictions if provided.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    # Plot training data\n",
        "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "\n",
        "    # Plot test data\n",
        "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "\n",
        "    if predictions is not None:\n",
        "        # Plot predictions on test data\n",
        "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "    # Display legend\n",
        "    plt.legend(prop={\"size\": 14})\n",
        "    plt.show()"
      ],
      "id": "bb1f72c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot decision boundaries for training and test sets\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model, X_train, y_train)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model, X_test, y_test)"
      ],
      "id": "0d98f130",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/ngo/Library/Python/3.12/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}