## Machine learning (ML)

Machine learning (ML) is a subset of artificial intelligence (AI) that enables machines to learn from experience and act automatically. ML algorithms can be broadly classified into three categories: supervised learning, unsupervised learning, and reinforcement learning.

### Supervised Learning

Supervised learning involves training ML algorithms on labeled datasets where both input and output parameters are known. The goal is to develop a regression or classification model that can be used on new datasets to generate predictions or draw conclusions. Supervised learning algorithms learn to map input features to output labels, allowing them to make predictions or classifications on new, unseen data.

Examples of supervised learning algorithms include:

1.  **Linear Regression**: A simple algorithm used to map the linear relationship between input features and a continuous target variable.
2.  **Logistic Regression**: An extension of linear regression used for classification tasks to estimate the likelihood that an instance belongs to a specific class.
3.  **Support Vector Machines (SVMs)**: Supervised learning algorithms that can perform classification and regression tasks by finding a hyperplane that best separates classes in feature space.
4.  **K-Nearest Neighbors (KNN)**: A non-parametric technique used for classification as well as regression by identifying the k most similar data points to a new data point and predicting the label of the new data point using the labels of those data points.
5.  **Decision Trees**: A type of supervised learning technique used for classification as well as regression by segmenting the data into smaller and smaller groups until each group can be classified or predicted with high degree of accuracy.
6.  **Random Forest**: An ensemble learning method that employs a set of decision trees to make predictions by aggregating predictions from individual trees.
7.  **Naive Bayes**: A probabilistic classifier based on Bayes’ theorem used for classification tasks by assuming that the features of a data point are independent of each other.

### Unsupervised Learning

Unsupervised learning involves training ML algorithms on unlabeled datasets where the data has yet to be categorized or labeled. The ML system itself learns to classify and process unlabeled data to learn from its inherent structure.

Examples of unsupervised learning algorithms include:

1.  **Clustering Algorithms**: Techniques used to group similar data points into clusters based on their features.
2.  **Association Rule Learning Algorithms**: Methods used to discover patterns and relationships between variables in the data.
3.  **Principal Component Analysis (PCA)**: A dimensionality reduction technique used to transform data into a lower-dimensional space while retaining as much variance as possible.

### Reinforcement Learning

Reinforcement learning involves training ML algorithms through ongoing interactions between an AI system and its environment. Algorithms receive numerical scores as rewards for generating decisions and outcomes, reinforcing positive interactions and behaviors over time.

Examples of reinforcement learning algorithms include:

1.  **Markov Decision Processes (MDPs)**: Model-based methods used to solve sequential decision-making problems.
2.  **Q-Learning**: A model-free method used to learn the value function of an action in a given state.
3.  **SARSA**: Another model-free method used to learn the value function of an action in a given state.
4.  **REINFORCE Algorithm**: A policy-based method used to learn the policy of an agent in a given environment.
5.  **Actor-Critic Algorithm**: A hybrid method used to learn both the policy and the value function of an agent in a given environment.

## Introduction Deep learning (DL)

Deep learning (DL) is a powerful subset of machine learning techniques that are inspired by the structure and function of the brain's neural networks. At the core of deep learning are artificial neural networks (ANNs), which are computational models composed of interconnected nodes called neurons. These neurons are organized into layers, allowing the network to learn hierarchical representations of data through a process called training.

ANNs are the fundamental building blocks of deep learning models. They consist of the following key components:

### Layers

-   **Input Layer**: This layer receives the input data, such as an image or a text sequence.
-   **Hidden Layers**: These are the intermediate layers that transform the input data into increasingly abstract representations. The depth of a neural network refers to the number of hidden layers it has.
-   **Output Layer**: This layer produces the final output, such as a classification or a prediction.

### Neurons and Activation Functions

Each layer in an ANN is composed of neurons, which are the basic computational units. Neurons receive input from the previous layer, perform a weighted sum of these inputs, and apply an activation function to produce an output that is passed to the next layer.

Common activation functions include:

-   **Sigmoid**: $$\sigma(x) = \frac{1}{1 + e^{-x}}$$ 
-   **Rectified Linear Unit (ReLU)**: $$f(x) = \max(0, x)$$ 
-   **Hyperbolic Tangent (Tanh)**: $$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$ 

### Training Neural Networks

Training an ANN involves the following key processes:

1.  **Forward Propagation**: Input data is passed through the network, and the output is computed.
2.  **Loss Function**: A loss function measures the difference between the predicted output and the true target value. Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.
3.  **Backpropagation**: The gradients of the loss function with respect to the weights are computed using the chain rule of calculus. This process is called backpropagation.
4.  **Optimization**: The weights of the network are updated using an optimization algorithm, such as Gradient Descent, to minimize the loss function. Popular variants of Gradient Descent include Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, Adam, and RMSprop.

## Applications of Deep Learning in Astrophysics

### 1. Observation of Interstellar Medium (ISM) and Molecular Clouds

-   **Data Reduction and Noise Filtering**: ML algorithms enhance observational data quality by reducing noise and improving the signal-to-noise ratio in data from radio, infrared, and optical telescopes.
    -   **Reference**: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. *Communication in Physical Sciences*, 10(1), 172-203. 
-   **Feature Extraction**: DL models, such as convolutional neural networks (CNNs), identify and classify structures within the ISM and molecular clouds, such as filaments and cores.
    -   **Reference**: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. *Communication in Physical Sciences*, 10(1), 172-203. 
-   **Anomaly Detection**: ML techniques detect unusual features or events within the ISM, potentially indicating new astrophysical phenomena.
    -   **Reference**: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. *Communication in Physical Sciences*, 10(1), 172-203. 

### 2. High-Mass Star Formation

-   **Data Analysis**: ML models analyze large datasets from observatories to identify regions of high-mass star formation by detecting specific spectral lines and emission features.
    -   **Reference**: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. *Monthly Notices of the Royal Astronomical Society*, 493(4), 4808-4815. 
-   **Pattern Recognition**: DL algorithms identify patterns and correlations in the spatial and temporal distribution of high-mass star-forming regions, helping to understand the underlying physical processes.
    -   **Reference**: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. *Monthly Notices of the Royal Astronomical Society*, 493(4), 4808-4815. 
-   **Predictive Modeling**: ML models predict the future evolution of star-forming regions based on current observations.
    -   **Reference**: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. *Monthly Notices of the Royal Astronomical Society*, 493(4), 4808-4815. 

### 3. Simulation of High-Mass Star Formation

-   **Accelerated Simulations**: ML techniques, such as generative adversarial networks (GANs), enhance the efficiency of high-mass star formation simulations by generating high-fidelity results from lower-resolution inputs.
    -   **Reference**: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. *Astronomy & Astrophysics*, 621, A51. 
-   **Parameter Space Exploration**: ML models explore the vast parameter space of star formation simulations to identify optimal conditions and initial configurations for high-mass star formation.
    -   **Reference**: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. *Astronomy & Astrophysics*, 621, A51. 

### 4. Astrochemistry

-   **Chemical Abundance Mapping**: ML models analyze spectral data to map the distribution of various molecules within molecular clouds and other astrophysical environments.
    -   **Reference**: Xue, B.-X., Barbatti, M., & Dral, P. (2020). Machine learning for absorption cross sections. *Journal of Physical Chemistry A*, 124(35), 7199-7210. 
-   **Reaction Network Analysis**: DL helps in understanding complex chemical networks by identifying key reactions and pathways in the formation and destruction of molecules in space.
    -   **Reference**: Villadsen, T., Ligterink, N. F. W., & Andersen, M. (2022). Predicting binding energies of astrochemically relevant molecules via machine learning. *Astronomy & Astrophysics*, 666, A45. 
-   **Anomaly Detection**: ML algorithms identify unusual chemical compositions or rare molecules, providing insights into unique astrochemical processes.
    -   **Reference**: Lee, K. L. K., Patterson, J., Burkhardt, A. M., Vankayalapati, V., & McCarthy, M. C. (2021). Machine learning of interstellar chemical inventories. *Astrophysical Journal Letters*, 917(1), L6. 

### 5. Stellar Feedback

-   **Simulation and Modeling**: DL accelerates simulations of stellar feedback processes, such as supernova explosions and stellar winds, by learning from high-resolution simulations and predicting outcomes for new scenarios.
    -   **Reference**: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. *Monthly Notices of the Royal Astronomical Society*, 493(4), 4808-4815. 
-   **Impact Analysis**: ML techniques assess the impact of stellar feedback on surrounding ISM and star formation by analyzing observational data and simulation outputs.
    -   **Reference**: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. *Monthly Notices of the Royal Astronomical Society*, 493(4), 4808-4815. 

### 6. Galaxy Formation

-   **Morphological Classification**: CNNs classify galaxies based on their shapes and structures from large-scale survey images, aiding in the study of galaxy formation and evolution.
    -   **Reference**: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. *Astronomy & Astrophysics*, 621, A51. 
-   **Merger Detection**: ML algorithms detect galaxy mergers and interactions in observational data and simulations, which are crucial for understanding galaxy formation processes.
    -   **Reference**: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. *Astronomy & Astrophysics*, 621, A51. 
-   **Predictive Modeling**: DL models predict the future evolution of galaxies based on current observations and simulations, helping to test theories of galaxy formation.
    -   **Reference**: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. *Astronomy & Astrophysics*, 621, A51. 